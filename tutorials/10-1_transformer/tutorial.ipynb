{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10-1: Attention is All You Need â€“ \"Building a Transformer Block\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Transformers, Self-Attention, Multi-Head Attention, and Positional Encoding\n",
    "\n",
    "## Objective\n",
    "The Transformer model (Vaswani et al., 2017) revolutionized Deep Learning by eschewing recurrence entirely in favor of **Attention mechanisms**. \n",
    "\n",
    "In this tutorial, we will not use the black-box `nn.Transformer`. Instead, we will build the architecture layer-by-layer to understand the math behind the magic.\n",
    "\n",
    "We will implement:\n",
    "1.  **Scaled Dot-Product Attention:** The mathematical core.\n",
    "2.  **Multi-Head Attention:** Running multiple attention mechanisms in parallel.\n",
    "3.  **Positional Encoding:** Injecting sequence order since we have no RNN loops.\n",
    "4.  **Encoder Block:** Assembling the full building block used in BERT and GPT.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Scaled Dot-Product Attention\n",
    "\n",
    "The core attention mechanism is defined as:\n",
    "\n",
    "$$ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V $$\n",
    "\n",
    "Where:\n",
    "* **Q (Query):** What I am looking for.\n",
    "* **K (Key):** What I match against.\n",
    "* **V (Value):** What I retrieve.\n",
    "* **$d_k$:** Dimension of the keys (used for scaling to prevent vanishing gradients in Softmax).\n",
    "\n",
    "Let's implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        # q, k, v: (Batch, Heads, Seq_Len, Dim)\n",
    "        \n",
    "        d_k = q.size(-1)\n",
    "        \n",
    "        # 1. Dot Product: (Batch, Heads, Seq, Dim) x (Batch, Heads, Dim, Seq)\n",
    "        # Result: (Batch, Heads, Seq, Seq) -> The \"Similarity Matrix\"\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # 2. Masking (Optional)\n",
    "        # Used in Decoder to hide future tokens, or to hide Padding tokens\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # 3. Softmax\n",
    "        attn_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 4. Weighted Sum of Values\n",
    "        output = torch.matmul(attn_weights, v)\n",
    "        \n",
    "        return output, attn_weights\n",
    "\n",
    "# Verification\n",
    "attention = ScaledDotProductAttention()\n",
    "# Dummy Data: Batch=1, Heads=1, Seq=5, Dim=64\n",
    "q = torch.randn(1, 1, 5, 64)\n",
    "k = torch.randn(1, 1, 5, 64)\n",
    "v = torch.randn(1, 1, 5, 64)\n",
    "out, weights = attention(q, k, v)\n",
    "\n",
    "print(f\"Output Shape: {out.shape} (Expected: 1, 1, 5, 64)\")\n",
    "print(f\"Weights Shape: {weights.shape} (Expected: 1, 1, 5, 5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Multi-Head Attention\n",
    "\n",
    "Instead of one single attention mechanism, we split the embedding dimension into multiple \"Heads.\" This allows the model to attend to different types of information (e.g., Head 1 focuses on grammar, Head 2 focuses on topic, etc.) simultaneously.\n",
    "\n",
    "**Logic:**\n",
    "1.  Project Input $X$ into $Q, K, V$ using linear layers.\n",
    "2.  **Split** into `n_heads`.\n",
    "3.  Run Scaled Dot-Product Attention on each head.\n",
    "4.  **Concatenate** the results.\n",
    "5.  Project back to original dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear Projections\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention()\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 1. Linear Projection + Split Heads\n",
    "        # Transform: (Batch, Seq, Dim) -> (Batch, Seq, Heads, d_k) -> (Batch, Heads, Seq, d_k)\n",
    "        q = self.w_q(q).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(k).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(v).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2. Attention\n",
    "        # Output: (Batch, Heads, Seq, d_k)\n",
    "        out, weights = self.attention(q, k, v, mask)\n",
    "        \n",
    "        # 3. Concatenate\n",
    "        # Transform: (Batch, Heads, Seq, d_k) -> (Batch, Seq, Heads, d_k) -> (Batch, Seq, Dim)\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # 4. Final Linear Layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Positional Encoding\n",
    "\n",
    "Since the Transformer processes all tokens in parallel (unlike an RNN), it has no inherent notion of order. \"Dog bites Man\" looks the same as \"Man bites Dog\" to the self-attention layer.\n",
    "\n",
    "We inject order by adding a **Positional Encoding (PE)** vector to the input embedding.\n",
    "\n",
    "$$ PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}}) $$\n",
    "$$ PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}}) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create matrix of [SeqLen, HiddenDim] representing the positional encoding for max_len inputs\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (Batch, Seq_Len, Dim)\n",
    "        # Add positional encoding to input embeddings\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return x\n",
    "\n",
    "# Visualization\n",
    "d_model = 128\n",
    "pos_enc = PositionalEncoding(d_model, max_len=100)\n",
    "dummy_input = torch.zeros(1, 100, d_model)\n",
    "output = pos_enc(dummy_input)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(output[0].numpy(), cmap='RdBu', aspect='auto')\n",
    "plt.title(\"Positional Encoding (100 positions, 128 dimensions)\")\n",
    "plt.xlabel(\"Embedding Dimension\")\n",
    "plt.ylabel(\"Sequence Position\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Look at the plot. Each row represents a position in the sequence. Each column is a dimension.\n",
    "You can see distinct wave patterns. This provides a unique, deterministic \"signature\" for every position that the model can learn to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Encoder Layer\n",
    "\n",
    "We now assemble the full **Transformer Encoder Layer**. \n",
    "\n",
    "**Architecture:**\n",
    "1.  **Multi-Head Attention**\n",
    "2.  **Add & Norm:** Residual Connection + Layer Normalization\n",
    "3.  **Feed-Forward Network:** Two linear layers with ReLU in between.\n",
    "4.  **Add & Norm:** Residual Connection + Layer Normalization\n",
    "\n",
    "This is the repeated block in BERT (12 or 24 layers of this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff)\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # 1. Attention Sublayer\n",
    "        # Residual Connection: x + Sublayer(x)\n",
    "        attn_output, _ = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        \n",
    "        # 2. Feed-Forward Sublayer\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Verify Assembly\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "seq_len = 20\n",
    "batch_size = 4\n",
    "\n",
    "layer = EncoderLayer(d_model, n_heads, d_ff)\n",
    "dummy_input = torch.randn(batch_size, seq_len, d_model)\n",
    "output = layer(dummy_input)\n",
    "\n",
    "print(f\"Input Shape: {dummy_input.shape}\")\n",
    "print(f\"Output Shape: {output.shape}\")\n",
    "print(\"Success! Dimensions are preserved (ready for stacking).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You have successfully implemented the core components of a Transformer from scratch.\n",
    "\n",
    "1.  **Q, K, V:** You learned how attention is just a \"soft\" database lookup.\n",
    "2.  **Multi-Head:** You saw how parallel heads allow the model to focus on different things at once.\n",
    "3.  **Positional Encoding:** You visualized how we inject order into a parallel architecture.\n",
    "4.  **Encoder Block:** You built the Lego brick that powers modern NLP (BERT, etc.).\n",
    "\n",
    "In the next tutorial, we will use the `transformers` library to fine-tune a pre-trained version of this architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
