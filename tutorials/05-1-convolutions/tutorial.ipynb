{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5-1: Building Blocks of Vision â€“ \"Convolutions from Scratch\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Convolutional Neural Networks (CNNs), Kernels, Stride, Padding, and Pooling\n",
    "\n",
    "## Objective\n",
    "Before using `nn.Conv2d` as a black box, we must understand exactly what happens inside. In this tutorial, we will:\n",
    "1.  **Implement Conv2d:** Write a naive 2D convolution function using nested loops to grasp the \"sliding window\" mechanics.\n",
    "2.  **Visualize Kernels:** Manually define edge detection filters (Sobel) and apply them to an image to see \"features\" emerge.\n",
    "3.  **Calculate Dimensions:** Write a utility to compute output shapes given input size, kernel size, stride, and padding (a classic exam topic!).\n",
    "4.  **Implement Pooling:** Write a naive Max Pooling function to understand downsampling.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Mechanics of Convolution\n",
    "\n",
    "A convolution involves sliding a small matrix (the **kernel** or filter) over a larger input matrix (the **image**) and computing the dot product at each position.\n",
    "\n",
    "**Key Parameters:**\n",
    "* **Kernel Size ($K$):** The width/height of the filter.\n",
    "* **Stride ($S$):** How many pixels the window moves each step.\n",
    "* **Padding ($P$):** Zero-pixels added around the border.\n",
    "\n",
    "Let's implement a naive version for a single channel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "def naive_conv2d(input_tensor, kernel, stride=1, padding=0):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        input_tensor: 2D tensor (H, W)\n",
    "        kernel: 2D tensor (K, K)\n",
    "        stride: int\n",
    "        padding: int\n",
    "    Returns:\n",
    "        output: 2D tensor\n",
    "    \"\"\"\n",
    "    # 1. Apply Padding\n",
    "    if padding > 0:\n",
    "        # Pad format: (left, right, top, bottom)\n",
    "        input_tensor = F.pad(input_tensor, (padding, padding, padding, padding))\n",
    "    \n",
    "    H_in, W_in = input_tensor.shape\n",
    "    K_h, K_w = kernel.shape\n",
    "    \n",
    "    # 2. Calculate Output Dimensions\n",
    "    # Formula: (W - K) / S + 1\n",
    "    H_out = (H_in - K_h) // stride + 1\n",
    "    W_out = (W_in - K_w) // stride + 1\n",
    "    \n",
    "    output = torch.zeros((H_out, W_out))\n",
    "    \n",
    "    # 3. Sliding Window Loop\n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            # Determine the window on the input\n",
    "            row_start = i * stride\n",
    "            row_end = row_start + K_h\n",
    "            col_start = j * stride\n",
    "            col_end = col_start + K_w\n",
    "            \n",
    "            # Extract patch\n",
    "            patch = input_tensor[row_start:row_end, col_start:col_end]\n",
    "            \n",
    "            # Element-wise multiply and sum (Dot Product)\n",
    "            output[i, j] = torch.sum(patch * kernel)\n",
    "            \n",
    "    return output\n",
    "\n",
    "print(\"Naive Convolution Defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Verification\n",
    "Let's compare our naive implementation against PyTorch's optimized `F.conv2d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random inputs\n",
    "input_test = torch.randn(5, 5)\n",
    "kernel_test = torch.randn(3, 3)\n",
    "\n",
    "# Our Run\n",
    "out_naive = naive_conv2d(input_test, kernel_test, stride=1, padding=0)\n",
    "\n",
    "# PyTorch Run (Requires expanding dimensions for Batch and Channel)\n",
    "# Input: (Batch=1, Channel=1, H, W)\n",
    "# Kernel: (OutChan=1, InChan=1, K, K)\n",
    "out_pytorch = F.conv2d(input_test.view(1, 1, 5, 5), kernel_test.view(1, 1, 3, 3), stride=1, padding=0)\n",
    "\n",
    "# Compare\n",
    "diff = torch.abs(out_naive - out_pytorch.squeeze()).sum()\n",
    "print(f\"Difference between Naive and PyTorch: {diff.item():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualizing Kernels (Edge Detection)\n",
    "\n",
    "In a CNN, the kernels are learned. But before deep learning, computer vision engineers manually designed kernels to detect edges. Let's see this in action.\n",
    "\n",
    "We will use the **Sobel Filter**, which approximates the derivative of the image intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Sobel Kernels\n",
    "sobel_x = torch.tensor([\n",
    "    [-1., 0., 1.],\n",
    "    [-2., 0., 2.],\n",
    "    [-1., 0., 1.]\n",
    "])\n",
    "\n",
    "sobel_y = torch.tensor([\n",
    "    [-1., -2., -1.],\n",
    "    [ 0.,  0.,  0.],\n",
    "    [ 1.,  2.,  1.]\n",
    "])\n",
    "\n",
    "# Load a sample image (using torchvision or creating a dummy one)\n",
    "try:\n",
    "    # Try to load a real image if available, else create synthetic\n",
    "    !wget -q -O sample.jpg https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\n",
    "    img_pil = Image.open(\"sample.jpg\").convert(\"L\") # Convert to grayscale\n",
    "    img_pil = img_pil.resize((200, 200))\n",
    "    img_tensor = torch.tensor(np.array(img_pil)).float()\n",
    "except:\n",
    "    print(\"Could not download image, using synthetic pattern.\")\n",
    "    img_tensor = torch.zeros(100, 100)\n",
    "    img_tensor[20:80, 20:80] = 255.0 # A white box in the middle\n",
    "\n",
    "# Apply Convolutions\n",
    "edges_x = naive_conv2d(img_tensor, sobel_x)\n",
    "edges_y = naive_conv2d(img_tensor, sobel_y)\n",
    "\n",
    "# Combine (Magnitude of gradient)\n",
    "edges_mag = torch.sqrt(edges_x**2 + edges_y**2)\n",
    "\n",
    "# Visualize\n",
    "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
    "axs[0].imshow(img_tensor, cmap='gray'); axs[0].set_title(\"Original\")\n",
    "axs[1].imshow(edges_x, cmap='gray'); axs[1].set_title(\"Vertical Edges (Sobel X)\")\n",
    "axs[2].imshow(edges_y, cmap='gray'); axs[2].set_title(\"Horizontal Edges (Sobel Y)\")\n",
    "axs[3].imshow(edges_mag, cmap='gray'); axs[3].set_title(\"Edge Magnitude\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Notice how `Sobel X` lights up vertical lines (where pixel values change from left-to-right) and `Sobel Y` lights up horizontal lines. \n",
    "\n",
    "**In a CNN:** The first layer filters often learn to look like these Sobel filters or Gabor filters automatically to detect boundaries!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Output Size Calculator\n",
    "\n",
    "A common source of bugs (and exam questions!) is calculating the shape of the tensor after a convolution.\n",
    "\n",
    "**Formula:** \n",
    "$$ W_{out} = \\lfloor \\frac{W_{in} - K + 2P}{S} \\rfloor + 1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_output_shape(input_size, kernel_size, stride, padding):\n",
    "    output_size = np.floor((input_size - kernel_size + 2 * padding) / stride) + 1\n",
    "    return int(output_size)\n",
    "\n",
    "# Test Cases from Slides (e.g., Slide 46)\n",
    "# Input: 32x32, Filter: 5x5, Stride: 1, Pad: 2\n",
    "in_s = 32; k = 5; s = 1; p = 2\n",
    "out_s = calculate_output_shape(in_s, k, s, p)\n",
    "print(f\"In: {in_s}, K: {k}, S: {s}, P: {p} -> Out: {out_s}\")\n",
    "print(f\"Does shape match input? {in_s == out_s} (Padding=2 preserves size for K=5)\")\n",
    "\n",
    "# Input: 7x7, Filter: 3x3, Stride: 1, Pad: 0 (Slide 26)\n",
    "in_s = 7; k = 3; s = 1; p = 0\n",
    "out_s = calculate_output_shape(in_s, k, s, p)\n",
    "print(f\"In: {in_s}, K: {k}, S: {s}, P: {p} -> Out: {out_s}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Max Pooling\n",
    "\n",
    "Pooling layers downsample the image, reducing computation and introducing translation invariance.\n",
    "**Max Pooling** selects the largest value in the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_maxpool2d(input_tensor, kernel_size=2, stride=2):\n",
    "    H_in, W_in = input_tensor.shape\n",
    "    \n",
    "    H_out = (H_in - kernel_size) // stride + 1\n",
    "    W_out = (W_in - kernel_size) // stride + 1\n",
    "    \n",
    "    output = torch.zeros((H_out, W_out))\n",
    "    \n",
    "    for i in range(H_out):\n",
    "        for j in range(W_out):\n",
    "            r_start = i * stride\n",
    "            c_start = j * stride\n",
    "            patch = input_tensor[r_start : r_start+kernel_size, c_start : c_start+kernel_size]\n",
    "            output[i, j] = torch.max(patch)\n",
    "            \n",
    "    return output\n",
    "\n",
    "# Apply to our edge-detected image\n",
    "pooled_img = naive_maxpool2d(edges_mag, kernel_size=4, stride=4)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1); plt.imshow(edges_mag, cmap='gray'); plt.title(f\"Before Pooling {edges_mag.shape}\")\n",
    "plt.subplot(1, 2, 2); plt.imshow(pooled_img, cmap='gray'); plt.title(f\"After MaxPool (4x4) {pooled_img.shape}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You have now implemented the core building blocks of a Convolutional Neural Network from scratch! \n",
    "* **Convolution** extracts local patterns (features).\n",
    "* **Padding** controls output size.\n",
    "* **Pooling** summarizes features and reduces dimensions.\n",
    "\n",
    "In the next tutorial, we will chain these together using PyTorch's optimized `nn.Conv2d` to classify images."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
