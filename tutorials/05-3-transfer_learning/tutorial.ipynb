{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5-3: Standing on the Shoulders of Giants â€“ \"Transfer Learning\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Transfer Learning, Feature Visualization, and Fine-Tuning\n",
    "\n",
    "## Objective\n",
    "Training a deep ConvNet from scratch requires massive datasets (like ImageNet with 1.2M images) and days of GPU time. Fortunately, we rarely need to do this. \n",
    "\n",
    "As discussed in the lecture (Slide 81), features learned on one task (e.g., classifying 1000 objects) often transfer well to other tasks. In this tutorial, we will:\n",
    "1.  **Load a Pre-trained ResNet-18:** A state-of-the-art model trained on ImageNet.\n",
    "2.  **Visualize Features:** Use \"hooks\" to peek inside the network and see how it processes an image (Early vs. Late layers).\n",
    "3.  **Perform Transfer Learning:** Adapt this powerful model to a new, smaller dataset (CIFAR-10) using the **Linear Probing** and **Fine-Tuning** strategies.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Pre-trained Model\n",
    "\n",
    "We will use `torchvision.models` to download a ResNet-18. \n",
    "* **`weights='IMAGENET1K_V1'`**: Tells PyTorch to download the weights learned from the ImageNet dataset.\n",
    "* **Architecture:** ResNet-18 is a deep CNN with residual connections (which we will study in depth later), but for now, treat it as a powerful feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import sys\n",
    "\n",
    "# Add parent directory to path to import utils\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_cifar10\n",
    "\n",
    "# 1. Pre-download Data\n",
    "download_cifar10()\n",
    "\n",
    "# Device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 1. Load Pre-trained Model\n",
    "# weights=ResNet18_Weights.IMAGENET1K_V1 is the modern way to specify pretrained=True\n",
    "try:\n",
    "    # Attempt to download automatically (may fail)\n",
    "    model = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n",
    "except Exception as e:\n",
    "    if not os.path.exists('../data/resnet18-f37072fd.pth'):\n",
    "        !wget -q -O '../data/resnet18-f37072fd.pth' https://download.pytorch.org/models/resnet18-f37072fd.pth\n",
    "    state_dict = torch.load('../data/resnet18-f37072fd.pth', weights_only=True)\n",
    "    \n",
    "    model = resnet18(weights=None)\n",
    "    model.load_state_dict(state_dict)\n",
    "    print(\"Loaded Pre-trained ImageNet Weights (Offline).\")\n",
    "    \n",
    "print(\"Model Loaded.\")\n",
    "# Let's look at the final layer (the head)\n",
    "print(\"Original Head:\", model.fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: What does a ConvNet see? (Feature Visualization)\n",
    "\n",
    "Before we modify the network, let's visualize the features it extracts. We will use a **Forward Hook** to save the output of intermediate convolutional layers.\n",
    "\n",
    "We expect:\n",
    "* **Early Layers:** Edges, colors, simple textures.\n",
    "* **Late Layers:** Abstract shapes, parts of objects (eyes, wheels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup Hooks\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register hooks on the first layer (conv1) and a deep layer (layer4)\n",
    "model.conv1.register_forward_hook(get_activation('conv1'))\n",
    "model.layer4[1].conv2.register_forward_hook(get_activation('layer4'))\n",
    "\n",
    "# 2. Load a sample image\n",
    "try:\n",
    "    !wget -q -O dog.jpg https://raw.githubusercontent.com/pytorch/hub/master/images/dog.jpg\n",
    "    img_pil = Image.open(\"dog.jpg\")\n",
    "except:\n",
    "    print(\"Could not download, using noise.\")\n",
    "    img_pil = Image.fromarray(np.uint8(np.random.rand(224,224,3)*255))\n",
    "\n",
    "# Preprocess for ResNet (Resize, Normalize)\n",
    "preprocess = ResNet18_Weights.IMAGENET1K_V1.transforms()\n",
    "img_tensor = preprocess(img_pil).unsqueeze(0) # Add batch dim\n",
    "\n",
    "# 3. Forward Pass\n",
    "model.eval()\n",
    "out = model(img_tensor)\n",
    "\n",
    "# 4. Visualize\n",
    "def plot_features(layer_name, num_filters=6):\n",
    "    act = activation[layer_name].squeeze()\n",
    "    fig, axarr = plt.subplots(1, num_filters, figsize=(15, 3))\n",
    "    fig.suptitle(f\"Features from {layer_name}\", fontsize=16)\n",
    "    for idx in range(num_filters):\n",
    "        axarr[idx].imshow(act[idx], cmap='viridis')\n",
    "        axarr[idx].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "plt.imshow(img_pil); plt.title(\"Input Image\"); plt.axis('off'); plt.show()\n",
    "plot_features('conv1')\n",
    "plot_features('layer4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Notice the resolution difference! \n",
    "* **`conv1`** is high-resolution ($112\\times112$) and retains spatial details (edges of the dog).\n",
    "* **`layer4`** is low-resolution ($7\\times7$). It looks like a blurry heatmap. Each pixel here represents a complex concept found in a large region of the original image (the Receptive Field!).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Transfer Learning (Linear Probing)\n",
    "\n",
    "We want to classify **CIFAR-10** images (10 classes), but ResNet-18 was built for ImageNet (1000 classes). \n",
    "\n",
    "**Strategy 1 (Slide 86):** Freeze the backbone (treat it as a fixed feature extractor) and train *only* the final linear layer. This is fast and effective for small datasets.\n",
    "\n",
    "**Steps:**\n",
    "1.  Set `requires_grad = False` for all parameters.\n",
    "2.  Replace `model.fc` with a new `nn.Linear(512, 10)`.\n",
    "3.  Train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Freeze Backbone\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. Replace Head (New layer automatically has requires_grad=True)\n",
    "num_ftrs = model.fc.in_features # 512 for ResNet18\n",
    "model.fc = nn.Linear(num_ftrs, 10) # CIFAR-10 has 10 classes\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# 3. Data Loaders (CIFAR-10)\n",
    "# ResNet expects 224x224 input usually, but we can use smaller if we want speed.\n",
    "# However, CIFAR is 32x32. ResNet downsizes by 32x.\n",
    "# If we input 32x32, the final feature map is 1x1. This works!\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224), # Resize to match ResNet expectation (better accuracy)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='../data', train=True, download=True, transform=transform_train)\n",
    "# Use subset for tutorial speed (500 images)\n",
    "train_subset, _ = torch.utils.data.random_split(trainset, [500, 49500])\n",
    "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 4. Train Only Head\n",
    "print(\"Params to learn:\")\n",
    "params_to_update = []\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad == True:\n",
    "        params_to_update.append(param)\n",
    "        print(\"\\t\", name)\n",
    "\n",
    "optimizer = optim.Adam(params_to_update, lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nStarting Linear Probing (Head Training)...\")\n",
    "model.train()\n",
    "for epoch in range(3): # Short run\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Fine-Tuning (The Golden Rule)\n",
    "\n",
    "We have trained the head. Now, to squeeze out maximum performance, we **unfreeze** the rest of the network and train everything together.\n",
    "\n",
    "**The Golden Rule (Slide 88):** Use a **much smaller learning rate** (e.g., 1/10th or 1/100th) for the backbone than the head. We don't want to destroy the beautiful pre-trained weights with large gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Unfreeze everything\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 2. Differential Learning Rates\n",
    "# Backbone gets 1e-4, Head gets 1e-3\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.layer1.parameters(), 'lr': 1e-5}, # Very small LR for early layers\n",
    "    {'params': model.layer2.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.layer3.parameters(), 'lr': 1e-4}, \n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-4},\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-3}      # Larger LR for the head\n",
    "])\n",
    "\n",
    "print(\"Starting Fine-Tuning (Backbone Unfrozen)...\")\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in trainloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} Loss: {running_loss/len(trainloader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "1.  **Transfer Learning** allows you to use state-of-the-art models on small datasets.\n",
    "2.  **Linear Probing (Freezing)** is a safe first step to align the new classifier with the pre-trained features.\n",
    "3.  **Fine-Tuning (Unfreezing)** adapts the features themselves to your specific domain (e.g., teaching the network to look for \"airplane wings\" specifically, rather than generic edges).\n",
    "\n",
    "**Pro Tip:** Always fine-tune with a low learning rate to avoid \"catastrophic forgetting\" of the ImageNet knowledge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
