{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7-1: Auditing AI – \"Bias in the Machine\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Algorithmic Bias, Word Embeddings, and AI Ethics\n",
    "\n",
    "## Objective\n",
    "We often think of AI models as objective mathematical tools. However, models trained on human data inevitably learn human biases. As discussed in the lecture, this can lead to harmful stereotypes being amplified in downstream applications (e.g., resume screening, search results).\n",
    "\n",
    "In this tutorial, we will **audit** a standard AI component—Word Embeddings—to measure and visualize gender bias. We will:\n",
    "1.  **Load GloVe Embeddings:** Use pre-trained vectors that represent words in a high-dimensional space.\n",
    "2.  **Measure Bias:** Mathematically calculate the association between neutral professions (e.g., \"programmer\", \"nurse\") and gendered words.\n",
    "3.  **Attempt a Fix:** Implement a geometric \"debiasing\" technique to remove this signal.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Getting the Data (GloVe)\n",
    "\n",
    "We will use the **GloVe (Global Vectors for Word Representation)** dataset. These vectors were trained on 6 billion tokens from Wikipedia and the Gigaword corpus.\n",
    "\n",
    "**Note:** We use `wget` to ensure this works on cluster compute nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import utility functions\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_glove_embeddings\n",
    "\n",
    "download_glove_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Loading Embeddings\n",
    "We will load the vectors into a Python dictionary mapping `word -> vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(path):\n",
    "    embeddings = {}\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings[word] = vector\n",
    "    return embeddings\n",
    "\n",
    "print(\"Loading embeddings into memory...\")\n",
    "embeddings = load_glove('../data/glove.6B.50d.txt')\n",
    "print(f\"Loaded {len(embeddings)} words.\")\n",
    "\n",
    "# Convert to PyTorch tensors for easier math later\n",
    "def get_vec(word):\n",
    "    return torch.tensor(embeddings[word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Vector Arithmetic (The Analogy Test)\n",
    "\n",
    "Word embeddings capture semantic meaning. We can perform algebra on words.\n",
    "The classic example: $Vector(King) - Vector(Man) + Vector(Woman) \\approx Vector(Queen)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest(target_vec, n=1, exclude_words=[]):\n",
    "    # Brute force search (slow but simple for tutorial)\n",
    "    # In production, use FAISS or ScaNN\n",
    "    scores = []\n",
    "    target_vec = target_vec / target_vec.norm() # Normalize\n",
    "    \n",
    "    # We'll search a subset of common words to speed this up, or just search all\n",
    "    # Let's search top 20,000 words for speed\n",
    "    search_space = list(embeddings.keys())[:20000]\n",
    "    \n",
    "    for word in search_space:\n",
    "        if word in exclude_words: continue\n",
    "        vec = torch.tensor(embeddings[word])\n",
    "        cosine_sim = torch.dot(target_vec, vec / vec.norm())\n",
    "        scores.append((word, cosine_sim.item()))\n",
    "    \n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    return scores[:n]\n",
    "\n",
    "# Test: King - Man + Woman = ?\n",
    "v_king = get_vec(\"king\")\n",
    "v_man = get_vec(\"man\")\n",
    "v_woman = get_vec(\"woman\")\n",
    "\n",
    "target = v_king - v_man + v_woman\n",
    "result = find_closest(target, exclude_words=[\"king\", \"man\", \"woman\"])\n",
    "print(f\"king - man + woman = {result[0][0]} (Score: {result[0][1]:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Bias Audit\n",
    "\n",
    "Now we investigate bias. We define a **Gender Axis** by subtracting the vector for \"he\" from \"she\" (or \"man\" from \"woman\").\n",
    "\n",
    "$$ g = v_{woman} - v_{man} $$\n",
    "\n",
    "We then project various profession words onto this axis. If the projection is positive, the model associates the word more with \"woman\". If negative, with \"man\". Ideally, neutral professions should be near 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the Gender Direction\n",
    "gender_direction = get_vec(\"woman\") - get_vec(\"man\")\n",
    "# Normalize it\n",
    "gender_direction = gender_direction / gender_direction.norm()\n",
    "\n",
    "# 2. List of professions to audit\n",
    "professions = [\n",
    "    \"programmer\", \"engineer\", \"scientist\", \"doctor\", \"architect\", \"boss\", \"leader\", # Historically male-skewed\n",
    "    \"nurse\", \"homemaker\", \"teacher\", \"artist\", \"secretary\", \"dancer\", \"receptionist\" # Historically female-skewed\n",
    "]\n",
    "\n",
    "# 3. Calculate projections\n",
    "projections = []\n",
    "for p in professions:\n",
    "    vec = get_vec(p)\n",
    "    # Project onto gender direction: dot product\n",
    "    score = torch.dot(vec / vec.norm(), gender_direction).item()\n",
    "    projections.append((p, score))\n",
    "\n",
    "# 4. Visualize\n",
    "projections.sort(key=lambda x: x[1])\n",
    "words, scores = zip(*projections)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['red' if s > 0 else 'blue' for s in scores]\n",
    "plt.barh(words, scores, color=colors)\n",
    "plt.axvline(0, color='black', linewidth=1)\n",
    "plt.title(f\"Projection onto 'Man' <---> 'Woman' Axis\")\n",
    "plt.xlabel(\"Gender Bias Score (Negative=Male, Positive=Female)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "You should see a clear pattern. Words like \"nurse\" and \"receptionist\" likely have strong positive scores (associated with \"woman\"), while \"engineer\" and \"boss\" have negative scores (associated with \"man\"). \n",
    "\n",
    "**Why this matters:** If we use these embeddings to rank resumes for a \"Programmer\" job, the model might mathematically penalize resumes containing female-coded language simply because the vector for \"programmer\" is far away from the vector for \"woman\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Geometric Debiasing\n",
    "\n",
    "We can attempt to fix this mathematically. We want to remove the component of the word vector that points in the gender direction.\n",
    "\n",
    "**The Formula:**\n",
    "$$ w_{debiased} = w - (w \\cdot g) \\times g $$\n",
    "Where $w$ is the word vector and $g$ is the unit gender direction vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neutralize(word, g_direction):\n",
    "    w = get_vec(word)\n",
    "    # Calculate component in direction of g\n",
    "    bias_component = torch.dot(w, g_direction) * g_direction\n",
    "    # Remove it\n",
    "    w_debiased = w - bias_component\n",
    "    return w_debiased / w_debiased.norm()\n",
    "\n",
    "# Apply neutralization to all professions\n",
    "debiased_projections = []\n",
    "for p in professions:\n",
    "    # Get neutralized vector\n",
    "    w_clean = neutralize(p, gender_direction)\n",
    "    # Re-calculate score\n",
    "    score = torch.dot(w_clean, gender_direction).item()\n",
    "    debiased_projections.append((p, score))\n",
    "\n",
    "# Visualize Before vs After\n",
    "plt.figure(figsize=(10, 8))\n",
    "y_pos = np.arange(len(words))\n",
    "\n",
    "plt.barh(y_pos - 0.2, scores, height=0.4, label='Original', color='gray', alpha=0.5)\n",
    "plt.barh(y_pos + 0.2, [s for _, s in debiased_projections], height=0.4, label='Debiased', color='green')\n",
    "\n",
    "plt.yticks(y_pos, words)\n",
    "plt.axvline(0, color='black')\n",
    "plt.legend()\n",
    "plt.title(\"Impact of Geometric Debiasing\")\n",
    "plt.xlabel(\"Gender Bias Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "The green bars should be extremely close to 0. We have successfully flattened the gender dimension for these words.\n",
    "\n",
    "**Critical Thinking:** \n",
    "Does this solve the problem? \n",
    "* **Yes:** The vector similarity between \"doctor\" and \"woman\" is now mathematically neutral.\n",
    "* **No:** Real-world bias is more complex than a single linear direction. There may be other hidden correlations (e.g., \"doctor\" might still be close to \"football\" while \"nurse\" is close to \"softball\"). \n",
    "\n",
    "Debiasing is an ongoing area of research, but auditing your models like this is the first step."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
