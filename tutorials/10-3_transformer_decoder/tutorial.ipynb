{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10-3: Generative Magic â€“ \"Text Generation with GPT-2\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Decoder Transformers, Autoregressive Generation, Sampling Strategies, and Prompting\n",
    "\n",
    "## Objective\n",
    "While BERT is a master of *understanding* (classification, NER), models like **GPT (Generative Pre-trained Transformer)** are masters of *creation*.\n",
    "\n",
    "GPT is a **Decoder-only** architecture. It is trained to predict the next token in a sequence given all previous tokens. This simple objective allows it to generate coherent essays, code, and stories.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1.  **Load GPT-2:** A smaller predecessor to the famous GPT-3/4, but conceptually identical.\n",
    "2.  **Implement the Generation Loop:** Manually write the code to generate text token-by-token.\n",
    "3.  **Compare Decoding Strategies:** Implement Greedy Search, Beam Search, and Top-K Sampling to see how they affect creativity.\n",
    "4.  **Prompt Engineering:** Use \"Few-Shot\" prompting to make the model solve logic puzzles without fine-tuning.\n",
    "\n",
    "**NOTE**: Run this notebook under the `Transformers Bundle` kernel instead of the class kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Robust Setup (Manual Download)\n",
    "\n",
    "As always, we download the model files manually to bypass cluster restrictions. We will use `gpt2` (117M parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config.json...\n",
      "Downloading pytorch_model.bin...\n",
      "Downloading vocab.json...\n",
      "Downloading merges.txt...\n",
      "Downloading tokenizer.json...\n",
      "Downloading tokenizer_config.json...\n",
      "GPT-2 Loaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define paths\n",
    "model_root = '../data/gpt2_local'\n",
    "os.makedirs(model_root, exist_ok=True)\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        exit_code = os.system(f\"wget -nc -q -O {save_path} {url}\")\n",
    "        if exit_code != 0:\n",
    "            print(f\"Error downloading {url}\")\n",
    "\n",
    "# Download GPT-2 Files\n",
    "base_hf_url = \"https://huggingface.co/gpt2/resolve/main/\"\n",
    "files_to_fetch = [\n",
    "    \"config.json\",\n",
    "    \"pytorch_model.bin\",\n",
    "    \"vocab.json\",\n",
    "    \"merges.txt\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\"\n",
    "]\n",
    "\n",
    "for filename in files_to_fetch:\n",
    "    download_file(base_hf_url + filename, os.path.join(model_root, filename))\n",
    "\n",
    "# Load Model and Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_root)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_root)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval() # Important: Turn off dropout!\n",
    "\n",
    "print(\"GPT-2 Loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Autoregressive Loop\n",
    "\n",
    "GPT generates text one token at a time. The output at step $t$ becomes the input at step $t+1$.\n",
    "\n",
    "Let's implement the most basic version: **Greedy Search**. At each step, we simply pick the token with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Greedy Search ---\n",
      "The secret to a happy life is to be able to do something that you love.\n",
      "\n",
      "I'm not saying that I'm a bad person, but I'm not saying that I'm a bad person. I'm just saying that I'm not a bad person.\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "def generate_greedy(prompt, max_length=50):\n",
    "    # 1. Encode Input\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # 2. Generation Loop\n",
    "    generated_ids = input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            # Forward pass\n",
    "            outputs = model(generated_ids)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Get logits for the LAST token only (Batch, Seq, Vocab)\n",
    "            next_token_logits = logits[:, -1, :]\n",
    "            \n",
    "            # Greedy: Pick highest probability\n",
    "            next_token_id = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "            \n",
    "            # Append to sequence\n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "            \n",
    "            # Stop if EOS token (optional, usually not needed for free generation)\n",
    "            if next_token_id.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "                \n",
    "    # 3. Decode\n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"--- Greedy Search ---\")\n",
    "print(generate_greedy(\"The secret to a happy life is\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "Greedy search often gets stuck in repetitive loops (e.g., \"I don't know. I don't know. I don't know.\"). This is because picking the *single* most likely word is not how humans write; we often pick words that are surprising but fitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Sampling Strategies (Creativity)\n",
    "\n",
    "To fix repetition, we use **Sampling**. Instead of `argmax`, we sample from the probability distribution.\n",
    "\n",
    "### 3.1 Temperature\n",
    "We divide logits by a temperature $T$ before softmax.\n",
    "* $T < 1$: Makes distribution sharper (more conservative).\n",
    "* $T > 1$: Makes distribution flatter (more random/creative).\n",
    "\n",
    "### 3.2 Top-K Sampling\n",
    "We only sample from the top $K$ most likely tokens. This prevents the model from choosing complete nonsense words from the \"tail\" of the distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- High Temperature (Creative/Chaotic) ---\n",
      "The secret to a happy life is to never be left out. By using the free will movement of people with very different ideologies, they are able to get what they want on a level that has no basis in fact; the more liberal ones tend to have the freedom of the freer\n",
      "\n",
      "--- Low Temperature (Safe/Boring) ---\n",
      "The secret to a happy life is to be able to be yourself.\n",
      "\n",
      "Photo Credit: Shutterstock.com\n",
      "\n",
      "The next time you're looking to get on a plane, make sure you're familiar with the rules of the game.\n",
      "\n",
      "Advertisement\n",
      "\n",
      "Advertisement\n",
      "\n",
      "I\n"
     ]
    }
   ],
   "source": [
    "def generate_sampling(prompt, max_length=50, temperature=1.0, top_k=0):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    generated_ids = input_ids\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length):\n",
    "            outputs = model(generated_ids)\n",
    "            next_token_logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Apply Temperature\n",
    "            next_token_logits = next_token_logits / temperature\n",
    "            \n",
    "            # Apply Top-K Filtering\n",
    "            if top_k > 0:\n",
    "                # Remove all tokens with a probability less than the last token of the top-k\n",
    "                indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]\n",
    "                next_token_logits[indices_to_remove] = -float('Inf')\n",
    "            \n",
    "            # Convert to Probabilities\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "            \n",
    "            # Sample\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "            \n",
    "            generated_ids = torch.cat([generated_ids, next_token_id], dim=-1)\n",
    "            \n",
    "    return tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"--- High Temperature (Creative/Chaotic) ---\")\n",
    "print(generate_sampling(\"The secret to a happy life is\", temperature=1.2, top_k=50))\n",
    "\n",
    "print(\"\\n--- Low Temperature (Safe/Boring) ---\")\n",
    "print(generate_sampling(\"The secret to a happy life is\", temperature=0.7, top_k=50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Few-Shot Prompting\n",
    "\n",
    "As mentioned in the lecture (Slide 84), large models like GPT-3 are \"Few-Shot Learners\". Even with our small GPT-2, we can demonstrate this effect.\n",
    "\n",
    "We will try to make the model perform **Sentiment Analysis** not by training it, but by giving it examples in the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Few-Shot Prompting ---\n",
      "\n",
      "Review: This movie was fantastic! Sentiment: Positive\n",
      "Review: I hated every minute of it. Sentiment: Negative\n",
      "Review: The acting was okay but the plot was boring. Sentiment: Negative\n",
      "Review: An absolute masterpiece of cinema. Sentiment: Positive\n",
      "Review: I fell asleep halfway through. Sentiment: Negative\n",
      "Review\n"
     ]
    }
   ],
   "source": [
    "# We format the prompt as a list of examples\n",
    "few_shot_prompt = \"\"\"\n",
    "Review: This movie was fantastic! Sentiment: Positive\n",
    "Review: I hated every minute of it. Sentiment: Negative\n",
    "Review: The acting was okay but the plot was boring. Sentiment: Negative\n",
    "Review: An absolute masterpiece of cinema. Sentiment: Positive\n",
    "Review: I fell asleep halfway through. Sentiment:\"\"\"\n",
    "\n",
    "print(\"--- Few-Shot Prompting ---\")\n",
    "# We generate just a few tokens to complete the pattern\n",
    "output = generate_greedy(few_shot_prompt, max_length=3)\n",
    "\n",
    "# Print only the new part\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You've just built a generative text pipeline!\n",
    "\n",
    "1.  **Generation Loop:** You saw how LMs generate text autoregressively.\n",
    "2.  **Sampling:** You learned that `Temperature` and `Top-K` control the trade-off between coherence and creativity.\n",
    "3.  **Prompting:** You saw that you can \"program\" a language model just by giving it text examples, without updating any weights (Zero/Few-Shot learning)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers Bundle",
   "language": "python",
   "name": "transformers-bundle-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
