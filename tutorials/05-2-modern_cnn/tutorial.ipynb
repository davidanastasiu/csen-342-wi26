{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5-2: The Modern CNN Architect â€“ \"From LeNet to ResNet\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** CNN Architectures, VGG Blocks, and Receptive Fields\n",
    "\n",
    "## Objective\n",
    "Deep Learning is not just about stacking random layers. It is about identifying and repeating successful **design patterns**.\n",
    "\n",
    "In this tutorial, we will trace the evolution of CNN architectures:\n",
    "1.  **LeNet-5 (1998):** The classic \"Sandwich\" architecture (Conv-Pool-Conv-Pool).\n",
    "2.  **VGG (2014):** The birth of the \"Modular Block\" design pattern (stacks of $3\\times3$ convolutions).\n",
    "3.  **Receptive Field:** We will calculate exactly how much of the input image a neuron \"sees,\" explaining why deeper networks are better at recognizing global shapes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Classic (LeNet-5)\n",
    "\n",
    "Yann LeCun's LeNet-5 (1998) was designed for handwriting recognition (MNIST). It established the standard pattern: **Convolution $\\to$ Activation $\\to$ Pooling**.\n",
    "\n",
    "**Architecture from Lecture Slide 8:**\n",
    "* Input: $32\\times32$ grayscale image\n",
    "* C1: Conv (6 filters, $5\\times5$, stride 1)\n",
    "* S2: Avg Pool ($2\\times2$, stride 2) *Note: Modern versions often use MaxPool*\n",
    "* C3: Conv (16 filters, $5\\times5$, stride 1)\n",
    "* S4: Avg Pool ($2\\times2$, stride 2)\n",
    "* C5: Fully Connected (120 units)\n",
    "* F6: Fully Connected (84 units)\n",
    "* Output: 10 units (Digits 0-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        # 1. Feature Extractor (The \"Sandwich\")\n",
    "        self.features = nn.Sequential(\n",
    "            # C1: Input 32x32 -> 28x28 (Filter 5x5 removes 4 pixels)\n",
    "            nn.Conv2d(1, 6, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            # S2: 28x28 -> 14x14\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # C3: 14x14 -> 10x10\n",
    "            nn.Conv2d(6, 16, kernel_size=5),\n",
    "            nn.ReLU(),\n",
    "            # S4: 10x10 -> 5x5\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # 2. Classifier (The MLP Head)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            # Input size: 16 channels * 5 * 5 pixels = 400 features\n",
    "            nn.Linear(16 * 5 * 5, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Verify Dimensions\n",
    "dummy_input = torch.randn(1, 1, 32, 32)\n",
    "model = LeNet5()\n",
    "out = model(dummy_input)\n",
    "print(f\"Input Shape: {dummy_input.shape}\")\n",
    "print(f\"Output Shape: {out.shape} (Expected: 1, 10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Modular Architect (VGG)\n",
    "\n",
    "In 2014, the VGG network introduced a key insight: **Small filters ($3\\times3$) stacked deeply are better than large filters.**\n",
    "\n",
    "Instead of writing every layer manually, VGG introduced the concept of the **VGG Block**: a sequence of convolutions followed by one pooling layer. This allows us to write code that *generates* the network.\n",
    "\n",
    "**A VGG Block:**\n",
    "1.  Conv3x3 (padding=1 to keep size)\n",
    "2.  ReLU\n",
    "3.  Conv3x3 (padding=1)\n",
    "4.  ReLU\n",
    "5.  MaxPool (halves the size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_block(in_channels, out_channels, num_convs):\n",
    "    layers = []\n",
    "    for _ in range(num_convs):\n",
    "        layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))\n",
    "        layers.append(nn.ReLU())\n",
    "        # The output of this layer becomes input for the next (in the loop)\n",
    "        in_channels = out_channels\n",
    "        \n",
    "    layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class MiniVGG(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=10):\n",
    "        super(MiniVGG, self).__init__()\n",
    "        \n",
    "        # Modular Construction\n",
    "        self.features = nn.Sequential(\n",
    "            vgg_block(in_channels, 32, 2),  # Block 1: 2 convs, 32 filters\n",
    "            vgg_block(32, 64, 2),           # Block 2: 2 convs, 64 filters\n",
    "            vgg_block(64, 128, 2)           # Block 3: 2 convs, 128 filters\n",
    "        )\n",
    "        \n",
    "        # Calculate Flattened Size dynamically\n",
    "        # Input 32x32 -> Pool -> 16x16 -> Pool -> 8x8 -> Pool -> 4x4\n",
    "        self.flat_dim = 128 * 4 * 4\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.flat_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5), # VGG heavily uses dropout\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Verify Dimensions for CIFAR-10 input (3 channels)\n",
    "dummy_cifar = torch.randn(1, 3, 32, 32)\n",
    "vgg_model = MiniVGG()\n",
    "out_vgg = vgg_model(dummy_cifar)\n",
    "print(f\"VGG Output Shape: {out_vgg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Receptive Field (Why deeper is better)\n",
    "\n",
    "Why do we stack layers? Why not just use one giant convolution?\n",
    "\n",
    "**The Concept:**\n",
    "Each neuron in a feature map only looks at a small patch of the previous layer. However, that patch looked at a patch of the layer before it. As we go deeper, the \"Receptive Field\" (the area of the original image that influences the neuron) grows.\n",
    "\n",
    "* **Layer 1 ($3\\times3$):** Sees $3\\times3$ pixels.\n",
    "* **Layer 2 ($3\\times3$):** Sees $5\\times5$ pixels (because the inputs are overlapping).\n",
    "* **Layer 3 ($3\\times3$):** Sees $7\\times7$ pixels.\n",
    "\n",
    "Let's visualize this growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rf(layers_description):\n",
    "    # Formula for RF growth: RF_prev + (Kernel - 1) * Stride_total\n",
    "    rf = 1\n",
    "    stride_total = 1\n",
    "    \n",
    "    print(f\"{'Layer':<15} | {'Kernel':<8} | {'Stride':<8} | {'Receptive Field'}\")\n",
    "    print(\"-\"*55)\n",
    "    \n",
    "    for name, k, s in layers_description:\n",
    "        # RF math\n",
    "        added_rf = (k - 1) * stride_total\n",
    "        rf += added_rf\n",
    "        stride_total *= s\n",
    "        print(f\"{name:<15} | {k:<8} | {s:<8} | {rf}\")\n",
    "\n",
    "# Define LeNet-like structure\n",
    "lenet_layers = [\n",
    "    (\"Conv1\", 5, 1),\n",
    "    (\"Pool1\", 2, 2),\n",
    "    (\"Conv2\", 5, 1),\n",
    "    (\"Pool2\", 2, 2)\n",
    "]\n",
    "\n",
    "# Define VGG-like structure (stacking 3x3s)\n",
    "vgg_layers = [\n",
    "    (\"Conv1_1\", 3, 1),\n",
    "    (\"Conv1_2\", 3, 1),\n",
    "    (\"Pool1\", 2, 2),\n",
    "    (\"Conv2_1\", 3, 1),\n",
    "    (\"Conv2_2\", 3, 1),\n",
    "    (\"Pool2\", 2, 2),\n",
    "    (\"Conv3_1\", 3, 1),\n",
    "    (\"Conv3_2\", 3, 1)\n",
    "]\n",
    "\n",
    "print(\"LeNet Receptive Field Growth:\")\n",
    "calculate_rf(lenet_layers)\n",
    "\n",
    "print(\"\\nVGG Receptive Field Growth:\")\n",
    "calculate_rf(vgg_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Observe how quickly the Receptive Field grows in VGG compared to LeNet. By the end of the VGG block, a single neuron \"sees\" a huge chunk of the image (often larger than the object itself!). This allows late layers to recognize global concepts like \"Cat\" or \"Car,\" while early layers only recognize local edges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
