{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 4-1: The Art of Initialization & Normalization\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Weight Initialization, Vanishing/Exploding Activations, and Batch Normalization\n",
    "\n",
    "## Objective\n",
    "Training deep neural networks is difficult because of the **Vanishing** or **Exploding** gradient problem. As signals pass through many layers, their variance can either shrink to zero (silence) or grow to infinity (instability).\n",
    "\n",
    "In this tutorial, we will visualize this phenomenon and apply the standard fixes from industry:\n",
    "1.  **Visualize Activations:** Monitor the distribution of data as it passes through a deep network.\n",
    "2.  **Bad Initialization:** See what happens when weights are initialized too large ($\\sigma=1$) or too small ($\\sigma=0.01$).\n",
    "3.  **Kaiming Initialization:** Apply the mathematical fix derived by Kaiming He to keep variance constant.\n",
    "4.  **Batch Normalization:** Apply the architectural fix that forces layer inputs to be stable, making initialization less critical.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Monitoring Activation Statistics\n",
    "\n",
    "We need a way to peek inside the network. We will create a Deep MLP that stores the `mean` and `std` (standard deviation) of the activations at every layer during the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Import utility functions\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "from utils import download_fashion_mnist\n",
    "\n",
    "# Device config\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class ActivationMonitorNet(nn.Module):\n",
    "    def __init__(self, depth=10, hidden_dim=100, use_bn=False):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.use_bn = use_bn\n",
    "        self.activation_stats = [] # To store (mean, std) for visualization\n",
    "        \n",
    "        # Create a deep network\n",
    "        for i in range(depth):\n",
    "            # Input layer handles 784 features, others handle hidden_dim\n",
    "            input_dim = 28*28 if i == 0 else hidden_dim\n",
    "            \n",
    "            # Define blocks: FC -> [BN] -> ReLU\n",
    "            block = nn.Sequential()\n",
    "            block.add_module(f'linear_{i}', nn.Linear(input_dim, hidden_dim))\n",
    "            \n",
    "            if use_bn:\n",
    "                # Batch Norm is typically applied BEFORE the activation (Slide 22)\n",
    "                block.add_module(f'bn_{i}', nn.BatchNorm1d(hidden_dim))\n",
    "                \n",
    "            block.add_module(f'relu_{i}', nn.ReLU())\n",
    "            \n",
    "            self.layers.append(block)\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.activation_stats = [] # Clear previous stats\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = layer(x)\n",
    "            # Record statistics of the output of this layer\n",
    "            mean = x.mean().item()\n",
    "            std = x.std().item()\n",
    "            self.activation_stats.append((mean, std))\n",
    "            \n",
    "        return x\n",
    "\n",
    "    def init_weights(self, method, std=0.01):\n",
    "        # Helper to re-initialize weights easily\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                if method == 'normal':\n",
    "                    nn.init.normal_(m.weight, mean=0.0, std=std)\n",
    "                elif method == 'kaiming':\n",
    "                    nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "print(\"Network defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Initialization Problem\n",
    "\n",
    "We will feed a batch of data through the network and observe the **standard deviation** of activations at each layer. \n",
    "* Ideally, `std` should remain stable (around 1.0) so the signal propagates.\n",
    "* If `std` drops to 0, the signal has vanished.\n",
    "* If `std` explodes, the signal has saturated or blown up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load one batch of data for testing\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "download_fashion_mnist()\n",
    "trainset = torchvision.datasets.FashionMNIST(root='../data', train=True, download=False, transform=transform)\n",
    "loader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
    "dummy_batch, _ = next(iter(loader))\n",
    "dummy_batch = dummy_batch.to(device)\n",
    "\n",
    "def visualize_flow(model, title):\n",
    "    # Run forward pass\n",
    "    model.to(device)\n",
    "    with torch.no_grad():\n",
    "        _ = model(dummy_batch)\n",
    "    \n",
    "    # Extract stats\n",
    "    means, stds = zip(*model.activation_stats)\n",
    "    layers = range(len(model.activation_stats))\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(layers, means, 'o-', label='Mean')\n",
    "    plt.title(f\"{title} - Activations Mean\")\n",
    "    plt.xlabel(\"Layer Depth\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(layers, stds, 'o-', color='orange', label='Std Dev')\n",
    "    plt.title(f\"{title} - Activations Std Dev\")\n",
    "    plt.xlabel(\"Layer Depth\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Experiment 1: Small Weights (std=0.01)\n",
    "model_small = ActivationMonitorNet(depth=10)\n",
    "model_small.init_weights('normal', std=0.01)\n",
    "visualize_flow(model_small, \"Small Init (std=0.01)\")\n",
    "\n",
    "# Experiment 2: Large Weights (std=1.0)\n",
    "model_large = ActivationMonitorNet(depth=10)\n",
    "model_large.init_weights('normal', std=1.0)\n",
    "visualize_flow(model_large, \"Large Init (std=1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "1.  **Small Init:** You should see the standard deviation collapse to 0 very quickly (Vanishing Activations). The network is essentially dead after a few layers.\n",
    "2.  **Large Init:** You might see the means shift significantly or variances explode, depending on the randomness. This leads to unstable gradients.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Fix (Kaiming Initialization)\n",
    "\n",
    "Kaiming He derived a formula specifically for ReLU networks. It sets the variance of weights to $\\frac{2}{n_{in}}$ to preserve the variance of activations through the ReLU nonlinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3: Kaiming He Initialization\n",
    "model_kaiming = ActivationMonitorNet(depth=10)\n",
    "model_kaiming.init_weights('kaiming')\n",
    "visualize_flow(model_kaiming, \"Kaiming/He Init\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Success!** The standard deviation should remain roughly stable (constant) across all 10 layers. This allows deep networks to train effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: The Stabilizer (Batch Normalization)\n",
    "\n",
    "Sometimes we cannot tune initialization perfectly. **Batch Normalization (BN)** is a technique that explicitly forces the activations of a layer to have mean 0 and variance 1.\n",
    "\n",
    "Let's assume we messed up and used the **Bad Initialization** (std=0.01), but this time we enable Batch Norm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4: Bad Init + Batch Norm\n",
    "model_bn = ActivationMonitorNet(depth=10, use_bn=True)\n",
    "model_bn.init_weights('normal', std=0.01) # Deliberately bad init\n",
    "visualize_flow(model_bn, \"Bad Init + Batch Norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Even though we initialized with terrible weights ($0.01$), Batch Norm forced the activations back to a healthy range (Std Dev ~ 1.0). This makes the network robust to initialization choices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Impact on Training\n",
    "\n",
    "Visualizing stats is nice, but does it actually help the model learn? Let's train three models for just 20 epochs and compare their loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_quick(model, name):\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    losses = []\n",
    "    \n",
    "    # Create a classification head for the final layer output\n",
    "    # Our model outputs 100 features, we need 10 classes\n",
    "    head = nn.Linear(100, 10).to(device)\n",
    "    # We need to add head parameters to optimizer\n",
    "    optimizer.add_param_group({'params': head.parameters()})\n",
    "    \n",
    "    model.train()\n",
    "    for i, (inputs, labels) in enumerate(loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        features = model(inputs)\n",
    "        outputs = head(features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 10 == 0:\n",
    "            losses.append(loss.item())\n",
    "        if i > 200: break # Run for just 200 batches to see initial convergence\n",
    "            \n",
    "    return losses\n",
    "\n",
    "print(\"Training 'Bad Init'...\")\n",
    "loss_bad = train_quick(model_small, \"Bad Init\")\n",
    "\n",
    "print(\"Training 'Kaiming Init'...\")\n",
    "loss_kaiming = train_quick(model_kaiming, \"Kaiming Init\")\n",
    "\n",
    "print(\"Training 'Batch Norm' (with Bad Init)...\")\n",
    "loss_bn = train_quick(model_bn, \"Batch Norm\")\n",
    "\n",
    "# Plot Loss Curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(loss_bad, label=\"Bad Init (0.01)\")\n",
    "plt.plot(loss_kaiming, label=\"Kaiming Init\")\n",
    "plt.plot(loss_bn, label=\"Batch Norm (with Bad Init)\")\n",
    "plt.title(\"Training Convergence Speed\")\n",
    "plt.xlabel(\"Iterations (x10)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "1.  **Bad Init:** Likely gets stuck at a high loss (approx `ln(10) = 2.3`) because signal doesn't propagate.\n",
    "2.  **Kaiming Init:** Starts learning immediately.\n",
    "3.  **Batch Norm:** Also starts learning immediately and often converges **faster** than Kaiming init alone because it allows for higher learning rates and smoother gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
