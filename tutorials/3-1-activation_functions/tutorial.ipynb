{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 3-1: The Activation Function Zoo & The Vanishing Gradient\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Activation Functions, Derivatives, and Gradient Flow\n",
    "\n",
    "## Objective\n",
    "The choice of activation function can make or break a deep neural network. While **ReLU** is the standard default, modern architectures like Transformers (BERT, GPT, Llama) and EfficientNets rely on newer, smoother functions like **GELU** and **Swish (SiLU)**.\n",
    "\n",
    "In this tutorial, we will:\n",
    "1.  **Visualize the Zoo:** Plot the curves and *derivatives* of both classic and modern activation functions to understand their properties (saturation, linearity, smoothness).\n",
    "2.  **The Vanishing Gradient Experiment (The Problem):** Train a very deep (50-layer) network with default initialization to see why Sigmoid and Tanh historically failed for deep networks.\n",
    "3.  **The Fix (Initialization):** Learn how **Xavier** and **Kaiming (He)** initialization schemes solve the vanishing gradient problem, allowing signal to flow through deep networks.\n",
    "4.  **Domain Guide:** Discuss which activations are best suited for Images, Text (LLMs), Audio, and Scientific data.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Visualizing the Zoo\n",
    "\n",
    "We will look at three generations of activation functions:\n",
    "1.  **Classic:** Sigmoid, Tanh (The \"S-curves\")\n",
    "2.  **The ReLU Family:** ReLU, Leaky ReLU, ELU (The piecewise linear functions)\n",
    "3.  **Modern Smooth:** GELU, SiLU (Swish), Mish (The non-monotonic functions used in LLMs)\n",
    "\n",
    "### Why look at the derivative?\n",
    "During backpropagation, the gradient flows through the activation function via the chain rule:  \n",
    "$$ \\frac{\\partial L}{\\partial x} = \\frac{\\partial L}{\\partial y} \\cdot f'(x) $$\n",
    "If $f'(x)$ is close to 0 (saturation), the gradient vanishes, and the network stops learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define range of inputs\n",
    "x = torch.linspace(-5, 5, 200, requires_grad=True)\n",
    "\n",
    "def plot_activation(name, func, ax_f, ax_d):\n",
    "    # 1. Forward Pass\n",
    "    y = func(x)\n",
    "    \n",
    "    # 2. Backward Pass (Compute Derivative)\n",
    "    if x.grad is not None:\n",
    "        x.grad.zero_()\n",
    "    y.sum().backward()\n",
    "    grad = x.grad.clone()\n",
    "    \n",
    "    # 3. Plot Function\n",
    "    ax_f.plot(x.detach().numpy(), y.detach().numpy(), label=name, linewidth=2)\n",
    "    ax_f.set_title(\"Activation Function $f(x)$\")\n",
    "    ax_f.grid(True, alpha=0.3)\n",
    "    ax_f.legend()\n",
    "    \n",
    "    # 4. Plot Derivative\n",
    "    ax_d.plot(x.detach().numpy(), grad.detach().numpy(), label=f\"{name}'\", linewidth=2)\n",
    "    ax_d.set_title(\"Derivative $f'(x)$\")\n",
    "    ax_d.grid(True, alpha=0.3)\n",
    "    ax_d.legend()\n",
    "\n",
    "# Setup Plotting\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Group 1: The Classics ---\n",
    "plot_activation(\"Sigmoid\", torch.sigmoid, axs[0], axs[1])\n",
    "plot_activation(\"Tanh\", torch.tanh, axs[0], axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation (Classics):** \n",
    "* **Saturation:** Notice that for inputs $>2$ or $<-2$, the derivative (right plot) drops to almost **zero**. This is the \"Vanishing Gradient\" problem.\n",
    "* **Zero-Centered:** Tanh is zero-centered (outputs range -1 to 1), while Sigmoid is not (0 to 1). Zero-centering usually helps convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Group 2: The ReLU Family ---\n",
    "plot_activation(\"ReLU\", F.relu, axs[0], axs[1])\n",
    "plot_activation(\"Leaky ReLU\", F.leaky_relu, axs[0], axs[1])\n",
    "plot_activation(\"ELU\", F.elu, axs[0], axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation (ReLU Family):**\n",
    "* **No Saturation (Positive):** For $x > 0$, the derivative is exactly 1. This solves the vanishing gradient problem for positive inputs.\n",
    "* **Dead Neurons:** For standard ReLU, if $x < 0$, the derivative is exactly 0. The neuron stops learning entirely.\n",
    "* **Leaky/ELU:** These try to fix the \"dead neuron\" problem by allowing a small gradient for negative inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# --- Group 3: Modern Smooth (Transformers/LLMs) ---\n",
    "# SiLU (Swish) = x * sigmoid(x)\n",
    "plot_activation(\"SiLU (Swish)\", F.silu, axs[0], axs[1])\n",
    "\n",
    "# GELU (Gaussian Error Linear Unit) - Used in BERT/GPT\n",
    "plot_activation(\"GELU\", F.gelu, axs[0], axs[1])\n",
    "\n",
    "# Mish - A newer self-regularized non-monotonic function\n",
    "plot_activation(\"Mish\", F.mish, axs[0], axs[1])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation (Modern):**\n",
    "* **Smoothness:** Unlike ReLU, these functions have a smooth curve at $0$. This makes the optimization landscape smoother, which helps very deep networks (like Transformers) train better.\n",
    "* **Non-Monotonic:** Notice the slight \"dip\" below zero for negative inputs. This property has been experimentally shown to improve regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Vanishing Gradient Experiment (The Problem)\n",
    "\n",
    "To visualize the problem, we will simulate a very deep network (**50 Layers**). We will pass random data through it and measure the magnitude of the gradients at each layer.\n",
    "\n",
    "We will start with **default initialization**, which is not optimized for very deep networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepNet(nn.Module):\n",
    "    def __init__(self, activation_func, depth=50, hidden_dim=100, custom_init=False, act_name=''):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = activation_func\n",
    "        \n",
    "        for _ in range(depth):\n",
    "            layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "            # Apply custom initialization if requested (Part 3)\n",
    "            if custom_init:\n",
    "                if act_name in ['Sigmoid', 'Tanh']:\n",
    "                    nn.init.xavier_normal_(layer.weight)\n",
    "                elif act_name in ['ReLU', 'GELU']:\n",
    "                    nn.init.kaiming_normal_(layer.weight, nonlinearity='relu')\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = self.activation(layer(x))\n",
    "        return x\n",
    "\n",
    "def check_gradients(activation_name, activation_func, custom_init=False):\n",
    "    # 1. Init Model (50 Layers!)\n",
    "    model = DeepNet(activation_func, depth=50, custom_init=custom_init, act_name=activation_name)\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "    \n",
    "    # 2. Synthetic Data\n",
    "    device = next(model.parameters()).device\n",
    "    x = torch.randn(64, 100).to(device)\n",
    "    \n",
    "    # 3. Forward & Backward\n",
    "    output = model(x)\n",
    "    loss = output.mean()\n",
    "    loss.backward()\n",
    "    \n",
    "    # 4. Collect Gradients\n",
    "    grads = []\n",
    "    layers = []\n",
    "    for i, layer in enumerate(model.layers):\n",
    "        if layer.weight.grad is not None:\n",
    "            grad_mag = layer.weight.grad.abs().mean().item()\n",
    "            grads.append(grad_mag)\n",
    "            layers.append(i)\n",
    "        \n",
    "    return layers, grads\n",
    "\n",
    "# Run Experiment with Default Init\n",
    "layers, grads_sigmoid = check_gradients(\"Sigmoid\", torch.sigmoid, custom_init=False)\n",
    "_, grads_tanh = check_gradients(\"Tanh\", torch.tanh, custom_init=False)\n",
    "_, grads_relu = check_gradients(\"ReLU\", F.relu, custom_init=False)\n",
    "_, grads_gelu = check_gradients(\"GELU\", F.gelu, custom_init=True)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(layers, grads_sigmoid, label=\"Sigmoid (Default Init)\", marker='o')\n",
    "plt.plot(layers, grads_tanh, label=\"Tanh (Default Init)\", marker='x')\n",
    "plt.plot(layers, grads_relu, label=\"ReLU (Default Init)\", marker='s')\n",
    "plt.plot(layers, grads_gelu, label=\"GELU (Default Init)\", marker='^')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Layer Depth (0 = Input, 50 = Output)\")\n",
    "plt.ylabel(\"Average Gradient Magnitude (Log Scale)\")\n",
    "plt.title(\"The Vanishing Gradient: 50 Layers (Default Init)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion (Part 2)\n",
    "You should see gradients vanishing (the lines plunge downward to the left):\n",
    "* **Sigmoid:** Likely vanishes almost immediately to $10^{-20}$ or less.\n",
    "* **Tanh:** Even though it is zero-centered, for a 50-layer network, the default initialization causes activations to grow or shrink until they hit the saturation regions (-1 or 1), killing the gradient.\n",
    "* **ReLU:** Even ReLU might show decay or instability. Without proper scaling, the variance of the signal changes as it goes deeper, eventually dying out.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Fix (Proper Initialization)\n",
    "\n",
    "The solution is to initialize the weights such that the variance of activations stays roughly constant across layers. We use specific schemes developed for this purpose:\n",
    "\n",
    "1.  **Xavier (Glorot) Initialization:** Designed for **Sigmoid** and **Tanh**. It keeps the variance of input and output gradients similar.\n",
    "2.  **Kaiming (He) Initialization:** Designed for **ReLU** and **GELU**. It accounts for the fact that ReLU zeroes out half the activations.\n",
    "\n",
    "Let's re-run the experiment with `custom_init=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Experiment with Proper Init\n",
    "layers, grads_sigmoid_fix = check_gradients(\"Sigmoid\", torch.sigmoid, custom_init=True)\n",
    "_, grads_tanh_fix = check_gradients(\"Tanh\", torch.tanh, custom_init=True)\n",
    "_, grads_relu_fix = check_gradients(\"ReLU\", F.relu, custom_init=True)\n",
    "_, grads_gelu_fix = check_gradients(\"GELU\", F.gelu, custom_init=True)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(layers, grads_sigmoid_fix, label=\"Sigmoid (Xavier Init)\", marker='o')\n",
    "plt.plot(layers, grads_tanh_fix, label=\"Tanh (Xavier Init)\", marker='x')\n",
    "plt.plot(layers, grads_relu_fix, label=\"ReLU (He Init)\", marker='s')\n",
    "plt.plot(layers, grads_gelu_fix, label=\"GELU (He Init)\", marker='^')\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xlabel(\"Layer Depth (0 = Input, 50 = Output)\")\n",
    "plt.ylabel(\"Average Gradient Magnitude (Log Scale)\")\n",
    "plt.title(\"The Fix: 50 Layers with Proper Initialization\")\n",
    "plt.legend()\n",
    "plt.grid(True, which=\"both\", ls=\"--\", alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion (Part 3)\n",
    "Compare this plot to the previous one:\n",
    "* **ReLU / GELU:** The lines should be much flatter (horizontal). The signal is preserved all the way back to the input.\n",
    "* **Tanh:** With Xavier initialization, Tanh becomes much more stable, although it can still struggle in extremely deep networks compared to ReLU.\n",
    "* **Sigmoid:** Even with Xavier init, Sigmoid usually struggles because its maximum derivative is only $0.25$. Multiplying $0.25$ fifty times results in a tiny number regardless of initialization.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Domain Guide (A Practical Summary)\n",
    "\n",
    "Choosing an activation function often depends on your data modality and network architecture.\n",
    "\n",
    "| Domain | Recommended Activation | Why? |\n",
    "| :--- | :--- | :--- |\n",
    "| **Computer Vision (CNNs)** | **ReLU** or **SiLU (Swish)** | ReLU is efficient and fast. SiLU is used in modern nets like EfficientNet/YOLOv5 for slightly better accuracy. |\n",
    "| **NLP / LLMs (Transformers)** | **GELU** or **SwishGLU** | GELU is smoother than ReLU, aiding optimization in very deep Transformers (BERT, GPT, Llama). |\n",
    "| **Audio / Waveforms** | **GELU** or **Tanh** | Waveforms oscillate; Tanh preserves sign, while GELU handles smoothness well. |\n",
    "| **Tabular / Scientific / Peptides** | **SELU** | Self-Normalizing Neural Networks (SNNs) typically use SELU to keep variance stable without Batch Normalization, which is helpful for noisy scientific data. |\n",
    "| **Reinforcement Learning** | **Tanh** or **ReLU** | Policy networks often output bounded actions (Tanh), while value networks use ReLU. |\n",
    "\n",
    "### Modern Trends\n",
    "In 2025+, **Swish (SiLU)** and **GELU** are becoming the defaults for almost everything except extremely lightweight mobile models (where ReLU is still used for speed)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "342wi26",
   "language": "python",
   "name": "342wi26"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
