{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 10-4: Vision Transformers – \"Image Patches as Tokens\"\n",
    "\n",
    "**Course:** CSEN 342: Deep Learning  \n",
    "**Topic:** Vision Transformers (ViTs), Patching, and Transfer Learning\n",
    "\n",
    "## Objective\n",
    "In the lecture, we learned that Vision Transformers (ViTs) introduce the idea of \"patches\" by dividing an image into smaller, fixed-size patches (e.g., 16x16 pixels). By using patches, ViTs can avoid convolutional layers, making them simpler and potentially more versatile than CNN-based models.\n",
    "\n",
    "In this tutorial, we will:\n",
    "\n",
    "1.  **Understand Patching:** See how these patches are then treated as tokens, similar to words in an NLP transformer.\n",
    "2.  **Fine-Tune ViT-Base:** Adapt a pre-trained ViT-Base model (12 layers, 768 hidden size, 86M params) to classify images.\n",
    "3.  **Perform Inference:** Build a pipeline to classify a new image.\n",
    "\n",
    "**NOTE**: Run this notebook under the `Transformers Bundle` kernel.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Robust Setup (The Offline Cache)\n",
    "\n",
    "Compute nodes often block direct downloads inside the notebook. We will manually download the pre-trained ViT model components to our local folder, similar to our NLP tutorials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading pytorch_model.bin...\n",
      "All files ready.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define paths\n",
    "data_root = '../data'\n",
    "model_root = '../data/vit_local'\n",
    "os.makedirs(data_root, exist_ok=True)\n",
    "os.makedirs(model_root, exist_ok=True)\n",
    "\n",
    "def download_file(url, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        print(f\"Downloading {os.path.basename(save_path)}...\")\n",
    "        os.system(f\"wget -nc -q -O {save_path} {url}\")\n",
    "\n",
    "# Download ViT-Base Model Files (Hugging Face Hub)\n",
    "base_hf_url = \"https://huggingface.co/google/vit-base-patch16-224-in21k/resolve/main/\"\n",
    "files_to_fetch = [\n",
    "    \"config.json\",\n",
    "    \"pytorch_model.bin\",\n",
    "    \"preprocessor_config.json\"\n",
    "]\n",
    "\n",
    "for filename in files_to_fetch:\n",
    "    download_file(base_hf_url + filename, os.path.join(model_root, filename))\n",
    "\n",
    "print(\"All files ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: The Image Processor (Patching)\n",
    "\n",
    "Since transformers lack a natural understanding of spatial arrangement, positional encodings are added to each patch embedding to provide spatial context. CNNs use local receptive fields, which focus on parts of the image incrementally, whereas transformers with global attention can analyze the entire image context at once.\n",
    "\n",
    "Each patch is linearly embedded into a fixed-size vector, making it compatible with the transformer's processing layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Size: (640, 480)\n",
      "Processed Tensor Shape: torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTImageProcessor\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "\n",
    "# Load from local path\n",
    "processor = ViTImageProcessor.from_pretrained(model_root)\n",
    "\n",
    "# Download a sample image to see the processor in action\n",
    "urllib.request.urlretrieve(\"http://images.cocodataset.org/val2017/000000039769.jpg\", \"sample.jpg\")\n",
    "image = Image.open(\"sample.jpg\")\n",
    "\n",
    "# The processor resizes the image to 224x224 and normalizes it\n",
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "print(f\"Original Image Size: {image.size}\")\n",
    "print(f\"Processed Tensor Shape: {inputs['pixel_values'].shape}\")\n",
    "\n",
    "# Note on the shape: [1, 3, 224, 224] means 1 image, 3 color channels, 224x224 pixels.\n",
    "# ViT-Base splits this into 16x16 patches. \n",
    "# (224 / 16) * (224 / 16) = 14 * 14 = 196 patches total!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The Dataset Class\n",
    "\n",
    "Vision transformers, unlike CNNs, generally require large amounts of data to perform well, especially for visual recognition tasks. ViTs are computationally intensive due to their use of self-attention, which scales quadratically with the number of patches.\n",
    "\n",
    "Because of this, ViTs benefit from transfer learning, where they are pretrained on large datasets and then fine-tuned for specific tasks. We will use a small, heavily-reduced subset of the CIFAR-10 dataset for this tutorial's speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR10Subset(Dataset):\n",
    "    def __init__(self, processor, train=True, samples=400):\n",
    "        # Download CIFAR10\n",
    "        self.dataset = torchvision.datasets.CIFAR10(root=data_root, train=train, download=True)\n",
    "        self.processor = processor\n",
    "        self.samples = samples\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image, label = self.dataset[idx]\n",
    "        # Convert image to the format expected by ViT\n",
    "        encoding = self.processor(images=image, return_tensors=\"pt\")\n",
    "        \n",
    "        return {\n",
    "            'pixel_values': encoding['pixel_values'].squeeze(0), # Remove batch dimension\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create Loaders\n",
    "train_ds = CIFAR10Subset(processor, train=True, samples=400)\n",
    "val_ds = CIFAR10Subset(processor, train=False, samples=100)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=8)\n",
    "cifar10_classes = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: The Model & Fine-Tuning\n",
    "\n",
    "On their own, ViTs are similar to encoder LLMs like BERT. The multi-headed self-attention mechanism enables the model to assess relationships between patches across the entire image.\n",
    "\n",
    "During training, the ViT is attached to a classification head which allows the transformer to use the encoded vectors for some task. This uses an extra learnable [class] embedding.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at ../data/vit_local and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Fine-Tuning (this might take a few mins)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:12<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss 2.2217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load Model from local path\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_root, \n",
    "    num_labels=10, # 10 classes in CIFAR-10\n",
    "    ignore_mismatched_sizes=True # We are replacing the original 21k classification head with a new 10-class head\n",
    ")\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "def train(epochs=1):\n",
    "    print(\"Starting Fine-Tuning (this might take a few mins)...\")\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch in tqdm(train_loader):\n",
    "            pixel_values = batch['pixel_values'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(pixel_values, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1}: Loss {avg_loss:.4f}\")\n",
    "\n",
    "train(epochs=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Inference\n",
    "\n",
    "Let's test the model on a new, unseen image from the validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Predictions ---\n",
      "True Label:      cat\n",
      "Predicted Label: cat\n"
     ]
    }
   ],
   "source": [
    "def predict(image):\n",
    "    model.eval()\n",
    "    inputs = processor(images=image, return_tensors=\"pt\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        prediction = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "    return cifar10_classes[prediction]\n",
    "\n",
    "# Grab a test image\n",
    "test_dataset = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True)\n",
    "test_image, true_label = test_dataset[0]\n",
    "\n",
    "print(\"--- Predictions ---\")\n",
    "print(f\"True Label:      {cifar10_classes[true_label]}\")\n",
    "print(f\"Predicted Label: {predict(test_image)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "You have successfully fine-tuned a Vision Transformer!\n",
    "\n",
    "**Important Note:**\n",
    "ViTs don't create images by generating tokens, this is typically done with Diffusion models, which leverage ViTs as an encoder backbone.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Transformers Bundle",
   "language": "python",
   "name": "transformers-bundle-cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
